| Parent Folder   | File/Script              | Block of Logic                    | Purpose/Description                                                              | Connected To                                                         | Contains __init__.py   |
|:----------------|:-------------------------|:----------------------------------|:---------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------|
| model_pipelines | 04_clv_projection.py     | Install Dependencies              | Installs necessary libraries like lifetimes and mlflow.                          | Intra-file: 04_clv_projection.py - Model Import and UDF Registration | Yes                    |
| model_pipelines | 04_clv_projection.py     | Model Import and UDF Registration | Imports pre-trained ML models and registers them as Spark UDFs.                  | Intra-file: 04_clv_projection.py - Define Table Schema               | Yes                    |
| model_pipelines | 04_clv_projection.py     | Define Table Schema               | Defines the schema for the table to store inference results.                     | Intra-file: 04_clv_projection.py - Batch Scoring                     | Yes                    |
| model_pipelines | 04_clv_projection.py     | Batch Scoring                     | Processes input data in batches and applies UDFs for prediction.                 | Intra-file: 04_clv_projection.py - Data Storage                      | Yes                    |
| model_pipelines | 04_clv_projection.py     | Data Storage                      | Stores the results in a defined table and optionally writes to external storage. |                                                                      | Yes                    |
| model_pipelines | 04_clv_projection.py     | Additional Operations (Optional)  | Includes any other additional data processing or logging.                        |                                                                      | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Dependencies                      | Installs necessary libraries and imports modules for modeling.                   | Intra-file: 03_monetary_modeling.py - Data Preparation               | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Data Preparation                  | Prepares data for the modeling process.                                          | Intra-file: 03_monetary_modeling.py - Train Spend Model              | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Train Spend Model                 | Trains a model to estimate the monetary value from future transactions.          | Intra-file: 03_monetary_modeling.py - Train Best Model               | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Train Best Model                  | Trains the best model based on hyperparameter tuning.                            | Intra-file: 03_monetary_modeling.py - Model Evaluation               | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Model Evaluation                  | Evaluates the model performance using metrics like RMSE, MAE.                    | Intra-file: 03_monetary_modeling.py - Final Training                 | Yes                    |
| model_pipelines | 03_monetary_modeling.py  | Final Training                    | Trains the model with the full dataset for final use.                            |                                                                      | Yes                    |
| model_pipelines | 02_frequency_modeling.py | Dependencies                      | Installs necessary libraries and imports modules for modeling.                   | Intra-file: 02_frequency_modeling.py - Data Preparation              | Yes                    |
| model_pipelines | 02_frequency_modeling.py | Data Preparation                  | Prepares and filters data for frequency modeling.                                | Intra-file: 02_frequency_modeling.py - Model Training                | Yes                    |
| model_pipelines | 02_frequency_modeling.py | Model Training                    | Trains and logs the frequency model using MLflow.                                | Intra-file: 02_frequency_modeling.py - Model Evaluation              | Yes                    |
| model_pipelines | 02_frequency_modeling.py | Model Evaluation                  | Evaluates the model performance and registers the final model.                   | Intra-file: 02_frequency_modeling.py - Final Training                | Yes                    |
| model_pipelines | 02_frequency_modeling.py | Final Training                    | Trains the final frequency model on the full dataset.                            |                                                                      | Yes                    |






| Parent Folder | File/Script | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description |
|---------------|-------------|----------------|----------------|-----------------------|-------------------------|-------------------------|---------------------|
| data_engineering_pipelines | 01_data_preparation.py | Initialize Spark Session | - Inputs: None<br>- Outputs: Spark session | `spark = SparkSession.builder.appName("CLV Data Preparation").getOrCreate()` | None | None | Initialize Spark session for data processing |
| data_engineering_pipelines | 01_data_preparation.py | Create View for Transaction Data | - Inputs: Raw transaction data<br>- Outputs: View `clv.vw_noam_txn_data_bronze` | `create_view_noam_txn_data_bronze()` | None | None | Create a view for filtered and preprocessed transaction data |
| data_engineering_pipelines | 01_data_preparation.py | Create Training Data Table | - Inputs: None<br>- Outputs: Table `clv.master_train_noam` | `create_training_data_table()` | None | None | Create a table to store training data |
| data_engineering_pipelines | 01_data_preparation.py | Generate Training Data | - Inputs: Processed transaction data<br>- Outputs: Processed training data in `clv.master_train_noam` | `generate_training_data()` | Depends on `create_view_noam_txn_data_bronze` and `create_training_data_table` | None | Process data to generate training dataset |
| data_engineering_pipelines | 01_data_preparation.py | Create Full Dataset Table | - Inputs: None<br>- Outputs: Table `clv.master_full_noam` | `create_full_dataset_table()` | None | None | Create a table to store the full dataset |
| data_engineering_pipelines | 01_data_preparation.py | Generate Full Dataset | - Inputs: Processed transaction data<br>- Outputs: Processed data in `clv.master_full_noam` | `generate_full_dataset()` | Depends on `create_view_noam_txn_data_bronze` | None | Process data to generate full dataset for analysis |
| data_engineering_pipelines | 01_data_preparation.py | Main Function Execution | - Inputs: None<br>- Outputs: Execution of data preparation functions | `main()` | Calls all above functions | None | Orchestrates the execution of data preparation steps |



| Parent Folder | File/Script | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description |
|---------------|-------------|----------------|----------------|-----------------------|-------------------------|-------------------------|---------------------|
| utils/api | make_ws_dir.py | Get experiment base path | - Inputs: base.yml<br>- Outputs: Experiment base path | `get_experiment_base_path()` | None | None | Retrieve the base path for experiments from configuration file |
| utils/api | make_ws_dir.py | Create directory in Databricks Workspace | - Inputs: Path<br>- Outputs: API response | `make_dir()` | None | None | Create a directory in Databricks Workspace using REST API |
| utils/api | make_ws_dir.py | Create directories for multiple regions | - Inputs: branch name<br>- Outputs: Directories in each region | `make_dirs()` | Depends on `get_experiment_base_path` and `make_dir` | None | Create directories for each region in Databricks Workspace |


| Parent Folder | File/Script | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description |
|---------------|-------------|----------------|----------------|-----------------------|-------------------------|-------------------------|---------------------|
| utils/api | permissions.py | Databricks API Request | - Inputs: Request parameters<br>- Outputs: API response | `api_request()` | None | None | Send requests to Databricks API |
| utils/api | permissions.py | Get model ID by name | - Inputs: Model name<br>- Outputs: Model ID | `get_model_id()` | None | Depends on `api_request` | Retrieve the model ID from its name using the API |
| utils/api | permissions.py | Create MLFlow model placeholder | - Inputs: Model name<br>- Outputs: Model ID | `create_model()` | None | Depends on `api_request` | Create a placeholder for a model in MLFlow registry |
| utils/api | permissions.py | Discover model configuration paths | - Inputs: Model config base path<br>- Outputs: Paths to model configs | `discover_model_paths()` | None | None | Discover paths to model configuration files |
| utils/api | permissions.py | Overwrite model permissions | - Inputs: Model path<br>- Outputs: API response | `overwrite_model_permission()` | None | Depends on `get_model_id`, `create_model`, `api_request` | Overwrite access control list for a model |
| utils/api | permissions.py | Overwrite permissions for all models | - Inputs: Model config base path<br>- Outputs: API response for each model | `overwrite_models_permission()` | None | Depends on `discover_model_paths`, `overwrite_model_permission` | Overwrite access control list for all discovered models |



| Parent Folder | File/Script | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description |
|---------------|-------------|----------------|----------------|-----------------------|-------------------------|-------------------------|---------------------|
| utils/pipelines | model_utils.py | Get Dummies Transformer | - Inputs: DataFrame<br>- Outputs: Transformed DataFrame | `GetDummies` class | None | None | Transformer for one-hot encoding using pandas.get_dummies |
| utils/pipelines | model_utils.py | Model Configuration Dataclass | - Inputs: Model config JSON<br>- Outputs: ModelConfig object | `ModelConfig` class | None | None | Dataclass for handling model configurations |
| utils/pipelines | model_utils.py | Calculate Tree Size in Bytes | - Inputs: Tree object<br>- Outputs: Size in bytes | `tree_nbytes()` | None | None | Calculate the size of decision tree arrays in bytes |
| utils/pipelines | model_utils.py | Get Estimator Size | - Inputs: Pipeline object<br>- Outputs: Model size stats | `get_estimator_size()` | None | None | Get the size of the model/estimator in a pipeline |


| Parent Folder | File/Script | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description |
|---------------|-------------|----------------|----------------|-----------------------|-------------------------|-------------------------|---------------------|
| utils/pipelines | save_to_blob.py | Get Commit ID from Notebook | - Inputs: Notebook path<br>- Outputs: Commit ID | `get_commit_id()` | None |




| Parent Folder  | File/Script       | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                         |
|----------------|-------------------|----------------|------------------|-----------------------|-------------------------|-------------------------|---------------------------------------------|
| conf/environments | dev.yml        | Environment Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Configuration for the development environment, including sample size and model stage |
| conf/environments | prod.yml       | Environment Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Configuration for the production environment, including sample size and model stage |
| conf/environments | staging.yml    | Environment Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Configuration for the staging environment, including sample size and model stage |





| Parent Folder  | File/Script           | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                                |
|----------------|-----------------------|----------------|------------------|-----------------------|-------------------------|-------------------------|----------------------------------------------------|
| conf/models/NOAM | clv_projection.yml | Model Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Placeholder for CLV projection model configuration |
| conf/models/NOAM | data_preparation.yml | Model Configuration | - Inputs: YAML configuration<br>- Outputs: Table mappings | None | None | None | Configuration for data preparation, defining source and destination tables |
| conf/models/NOAM | frequency_modeling.yml | Model Configuration | - Inputs: YAML configuration<br>- Outputs: Feature types, source and destination tables | None | None | None | Configuration for frequency modeling, including feature types and table mappings |
| conf/models/NOAM | monetary_modeling.yml | Model Configuration | - Inputs: YAML configuration<br>- Outputs: Feature types, source and destination tables | None | None | None | Configuration for monetary modeling, including feature types and table mappings |





| Parent Folder  | File/Script   | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                        |
|----------------|---------------|----------------|------------------|-----------------------|-------------------------|-------------------------|--------------------------------------------|
| conf | base.yml | Base Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Base configuration for the project, including paths, storage, database, and column names |





| Parent Folder  | File/Script       | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                               |
|----------------|-------------------|----------------|------------------|-----------------------|-------------------------|-------------------------|---------------------------------------------------|
| conf | deployment.yml | Deployment Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Configuration for deployment, including cluster specs, environments, workflows, and ACL |




| Parent Folder  | File/Script       | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                               |
|----------------|-------------------|----------------|------------------|-----------------------|-------------------------|-------------------------|---------------------------------------------------|
| conf | pool_config.json | Instance Pool Configuration | - Inputs: JSON configuration<br>- Outputs: Instance pool settings | None | None | None | Configuration for Databricks instance pool, specifying types and limits of nodes |




| Parent Folder  | File/Script       | Block of Logic | Inputs/Outputs   | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                               |
|----------------|-------------------|----------------|------------------|-----------------------|-------------------------|-------------------------|---------------------------------------------------|
| conf | templates.yml | Job Templates Configuration | - Inputs: None<br>- Outputs: YAML configuration | None | None | None | Configuration for job templates, including tasks, dependencies, and cluster specs |





| Parent Folder | File/Script    | Block of Logic | Inputs/Outputs | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity | Purpose/Description                          |
|---------------|----------------|----------------|----------------|-----------------------|-------------------------|-------------------------|----------------------------------------------|
| .dbx | project.json | DBX Project Configuration | - Inputs: JSON configuration<br>- Outputs: Project settings | None | None | None | Configuration for DBX project environments, including paths and artifact locations |





| Parent Folder   | File/Script                | Block of Logic                           | Inputs/Outputs                              | Key Functions/Classes                                       | Intra-File Connectivity  | Inter-File Connectivity | Purpose/Description                                             |
|-----------------|----------------------------|------------------------------------------|---------------------------------------------|-------------------------------------------------------------|--------------------------|-------------------------|-----------------------------------------------------------------|
| gda_clv/utils   | common.py                  | Singleton and Data Classes               | - Inputs: Various configuration files<br>- Outputs: Configured objects | SingletonMeta, FeatureStoreTableConfig, TargetsTableConfig, JobContext | None                     | None                    | Provides utility classes and methods for managing job contexts and configurations |
| gda_clv/utils   | config_utils.py            | Configuration Loading and Handling       | - Inputs: Configuration files<br>- Outputs: Configuration objects | BaseConfig, EnvConfig, ModelConfig, SuffixHandler, InputConfig, RegionConfig, ExternalDBConfig | None                     | None                    | Loads and processes configuration files for different environments and models |
| gda_clv/utils   | dbx_utils.py               | Job and Deployment Configuration Helpers | - Inputs: Configuration templates<br>- Outputs: Deployment configurations | JobBuilder, DiscoverHandler, JobFactory, JobHandler         | None                     | None                    | Assists in building and configuring jobs for deployment in various environments |
| gda_clv/utils   | feature_store_utils.py     | Feature Store Utilities                  | - Inputs: Feature data, table configurations<br>- Outputs: Feature tables, DataFrames | create_and_write_feature_table, Timeline, FeatureUpdate, DuplicatesHandler, FeatureUpdateConditions | None                     | None                    | Utilities for managing feature tables and data within Databricks Feature Store |
| gda_clv/utils   | job_gen.py                 | Job Generation Script                    | - Inputs: Configuration templates<br>- Outputs: Deployment configuration file | None                                                          | None                     | None                    | Generates deployment configuration for Databricks jobs based on templates |
| gda_clv/utils   | job_meta_checker.py        | Job Metadata Checker Script              | - Inputs: Context, commit IDs<br>- Outputs: Validation results | meta_check_commit                                             | None                     | None                    | Validates the presence of specific commit IDs in job metadata for accuracy |
| gda_clv/utils   | job_templates_scheduler_gen.py | Job Template Scheduler Generation    | - Inputs: Template configurations<br>- Outputs: Updated schedule in templates | get_all_job_schedule, get_updated_schedule                   | None                     | None                    | Dynamically generates and updates job schedules in template configurations |
| gda_clv/utils   | logger_utils.py            | Logger Utility                           | - Inputs: None<br>- Outputs: Configured logger | get_logger                                                     | None                     | None                    | Provides a utility function to configure and retrieve a standard logger |






| Parent Folder | File/Script    | Block of Logic             | Inputs/Outputs                                                                                                              | Key Functions/Classes                    | Intra-File Connectivity                                                                                                 | Inter-File Connectivity                                                                                             | Purpose/Description                                                                                                            |
|---------------|----------------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| utils         | config_utils.py| JSON to YAML Conversion    | - Inputs: JSON file paths<br>- Outputs: YAML files                                                                         | `json_to_yml()`                          | None                                                                                                                      | Utilized in data engineering pipelines for converting configuration files from JSON to YAML                        | Converts JSON configuration files to YAML format, facilitating easier configuration management in the `gda_clv` codebase.       |
| utils         | config_utils.py| Configuration Path Getter  | - Inputs: Command line arguments<br>- Outputs: Configuration file path                                                      | `get_conf_path()`                        | None                                                                                                                      | Used across the codebase where configuration paths are required, such as in job setup scripts.                        | Retrieves the path to the configuration file from command line arguments, aiding in dynamic configuration management.        |
| utils         | config_utils.py| Load Configuration         | - Inputs: Configuration file path<br>- Outputs: Configuration dictionary                                                    | `load_config()`                          | Used in `from_path()` methods in various configuration classes.                                                         | Referenced in different scripts and classes for loading configuration settings.                                      | Loads configuration settings from a specified file path, supporting various configuration management tasks.                 |
| utils         | config_utils.py| Command Line Arguments     | - Inputs: Command line arguments<br>- Outputs: Arguments dictionary                                                         | `get_args()`, `get_flags()`              | None                                                                                                                      | Widely used for parsing command line arguments in scripts across the `gda_clv` codebase.                             | Parses command line arguments, providing a standardized way to handle script parameters and flags.                           |
| utils         | config_utils.py| Notebook Context Getter    | - Inputs: Databricks utilities object<br>- Outputs: Notebook context dictionary                                             | `get_notebook_context()`                 | None                                                                                                                      | Potentially used in various scripts where notebook context or metadata is needed.                                    | Retrieves the context of a Databricks notebook, including user and path information.                                        |
| utils         | config_utils.py| Username Hash Generator    | - Inputs: Databricks utilities object<br>- Outputs: SHA-1 hash of username                                                  | `get_username_hash()`                    | None                                                                                                                      | Could be used for generating unique identifiers or paths based on the user running the notebook.                      | Generates a hash of the username, potentially for creating unique identifiers or paths in the codebase.                     |
| utils         | config_utils.py| Load Multiple Configs      | - Inputs: Paths to configuration files<br>- Outputs: Combined configuration dictionary                                      | `load_configs()`                         | None                                                                                                                      | May be referenced in scripts requiring combined configurations from multiple sources.                                | Loads and combines multiple configuration settings, supporting complex configuration scenarios.                          |
| utils         | config_utils.py| Base Configuration Class   | - Inputs: Configuration dictionary<br>- Outputs: BaseConfig instance                                                        | `BaseConfig`                             | Utilized in various configuration classes for inheritance.                                                              | Referenced in job context and other scripts for accessing base configurations.                                        | Base class for configurations, defining common properties and methods for managing project-wide settings.                   |
| utils         | config_utils.py| Environment Config Class   | - Inputs: Configuration dictionary<br>- Outputs: EnvConfig instance                                                         | `EnvConfig`                              | Inherits `Config` class                                                                                                  | Used in job setup and context scripts for accessing environment-specific configurations.                             | Manages configurations specific to different environments like development, staging, or production.                         |
| utils         | config_utils.py| Model Configuration Class  | - Inputs: Configuration dictionary<br>- Outputs: ModelConfig instance                                                       | `ModelConfig`                            | Inherits `Config` class, extended to handle model-specific configurations.                                             | Referenced in model training, evaluation, and inference scripts for model-related settings.                          | Handles configurations related to specific machine learning models, including features and target settings.                |
| utils         | config_utils.py| Suffix Handling Class      | - Inputs: Configuration settings<br>- Outputs: Updated table names with suffixes                                            | `SuffixHandler`                          | Utilized in `InputConfig` class for appending suffixes to table names.                                                 | May be used in scripts where dynamic suffixes are appended to database or table names based on context.               | Manages the appending of suffixes to database or table names, supporting dynamic naming conventions.                      |
| utils         | config_utils.py| Input Configuration Class  | - Inputs: Base, environment, and model configurations<br>- Outputs: InputConfig instance                                   | `InputConfig`                            | Uses `SuffixHandler` for handling table name suffixes.                                                                  | Referenced in data engineering and model training scripts for accessing input configurations.                         | Manages the configurations related to input data sources, facilitating data handling across different stages of the pipeline. |
| utils         | config_utils.py| Region Configuration Class | - Inputs: Model configuration path<br>- Outputs: RegionConfig instance                                                      | `RegionConfig`                           | Used to aggregate configurations across models within a region.                                                         | May be referenced in scripts that require region-wide configuration settings.                                         | Aggregates configurations across different models within a specific region, supporting regional analysis and operations.    |
| utils         | config_utils.py| External DB Config Class   | - Inputs: Configuration dictionary<br>- Outputs: ExternalDBConfig instance                                                  | `ExternalDBConfig`                       | None                                                                                                                      | Potentially used in scripts that connect to external databases, such as data extraction or integration processes.     | Manages the configurations for connecting to external databases, centralizing database connection settings.               |






| Parent Folder | File/Script            | Block of Logic                   | Inputs/Outputs                                                                                                                               | Key Functions/Classes                   | Intra-File Connectivity                               | Inter-File Connectivity                                                                                                      | Purpose/Description                                                                                                                                                               |
|---------------|------------------------|----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| utils         | feature_store_utils.py | Create and Write Feature Table   | - Inputs: FeatureStoreClient, DataFrame, feature table name, primary keys, timestamp column, description<br>- Outputs: FeatureTable         | `create_and_write_feature_table()`      | None                                                 | Used in `job_gen.py` for creating and writing to feature tables.                                                              | Creates a new feature table in the Databricks Feature Store and writes data to it, configuring primary and timestamp keys, along with a description.                               |
| utils         | feature_store_utils.py | Convert to Datetime              | - Inputs: Timestamp (date or datetime object)<br>- Outputs: Datetime object                                                                  | `to_datetime()`                         | None                                                 | Directly referenced in `agg_col()` within the same file for datetime conversions.                                             | Converts date or datetime objects to a standardized datetime object for consistent timestamp handling.                                                                              |
| utils         | feature_store_utils.py | Aggregate Column                 | - Inputs: SparkSession, Table name, Column name, Aggregation function<br>- Outputs: Aggregated value                                         | `agg_col()`                             | None                                                 | Utilized in `get_latest()` and `check_features_update_ts()` within the same file for aggregating column values.             | Aggregates a specified column in a table using a given aggregation function, returning the aggregated result.                                                                       |
| utils         | feature_store_utils.py | Get Latest Timestamp             | - Inputs: SparkSession, Table name, Column name<br>- Outputs: Latest timestamp value                                                         | `get_latest()`                          | Utilizes `agg_col()`                                  | Referenced in `get_inference_input()` and `FeatureUpdate` class within the same file for fetching the latest timestamp.    | Retrieves the latest timestamp from a specified column in a table, aiding in time-based data processing.                                                                           |
| utils         | feature_store_utils.py | Check Feature Table Existence    | - Inputs: FeatureStoreClient, Table name<br>- Outputs: Boolean indicating table existence                                                   | `check_feature_table_exists()`          | None                                                 | Called in `check_cold_start()` within the same file to verify the existence of feature tables.                                | Checks whether a given feature table exists in the Databricks Feature Store.                                                                                                        |
| utils         | feature_store_utils.py | Check Features Update Timestamp  | - Inputs: SparkSession, Table name, Timestamp column, Target update timestamp<br>- Outputs: Boolean indicating if update is needed          | `check_features_update_ts()`            | Utilizes `agg_col()` and `to_datetime()`              | Integral to `check_cold_start()` in the same file for assessing feature table update requirements based on timestamps.     | Determines if a feature table contains rows that need updating based on a target timestamp.                                                                                         |
| utils         | feature_store_utils.py | Check Cold Start Requirement     | - Inputs: SparkSession, FeatureStoreClient, Feature table name, Timestamp column, Target update timestamp<br>- Outputs: Boolean indication   | `check_cold_start()`                    | Utilizes `check_feature_table_exists()` and `check_features_update_ts()` | Invoked in `job_gen.py` to determine if a cold start process is necessary for feature tables.                              | Evaluates whether a cold start of feature data is required based on table existence and update timestamps.                                                                         |
| utils         | feature_store_utils.py | Query Feature Evaluation Period  | - Inputs: SparkSession, Source DB, Source table, Timestamp column, Period start and end<br>- Outputs: Temporary view                         | `query_feature_eval_period()`           | None                                                 | Not directly referenced in other files but essential for setting up temporary views in data processing pipelines.           | Creates a temporary view of a source table for a specified period based on timestamps, aiding in feature evaluation.                                                              |
| utils         | feature_store_utils.py | Timeline Class                   | - Inputs: Latest transaction timestamp, Training end timestamp, Training start timestamp<br>- Outputs: Timeline instance                    | `Timeline`                              | None                                                 | Used in `FeatureUpdate` class in the same file to manage update timelines.                                                   | Manages the timeline of feature and target tables updates, defining critical timestamps for data processing activities.                                                            |
| utils         | feature_store_utils.py | Get Inference Input DataFrame    | - Inputs: SparkSession, Feature table name, ID column, Timestamp column<br>- Outputs: DataFrame for inference input                          | `get_inference_input
()`                 | Utilizes `get_latest()`                               | Referenced in `job_gen.py` for preparing inference input data.                                                               | Retrieves the latest feature data from a feature table for use as input in machine learning inference.                                                                             |
| utils         | feature_store_utils.py | Duplicates Handler Class         | - Inputs: ID column name, Threshold<br>- Outputs: Modified DataFrame without duplicates                                                     | `DuplicatesHandler`                     | None                                                 | Could be used in data preparation or feature engineering scripts to manage duplicates, though not directly referenced.     | A class for handling duplicate records in data, ensuring data integrity and consistency.                                                                                            |
| utils         | feature_store_utils.py | Feature Update Conditions Class  | - Inputs: JobContext instance<br>- Outputs: FeatureUpdateConditions instance indicating various update conditions                           | `FeatureUpdateConditions`               | Utilizes `JobContext`                                | Directly used in `FeatureUpdate` class within the same file for managing update conditions.                                  | Manages conditions for updating feature data, considering aspects like job context and dataset characteristics.                                                                    |
| utils         | feature_store_utils.py | Feature Update Class             | - Inputs: FeatureUpdateConditions instance, Timeline instance, Features evaluation period<br>- Outputs: Feature update parameters           | `FeatureUpdate`                         | Utilizes `FeatureUpdateConditions` and `Timeline`     | Central to managing feature data updates, particularly in data engineering and feature store operations, but not directly referenced elsewhere. | Manages the process of updating feature data, considering update conditions and evaluation timelines.                                                                              |





| Parent Folder | File/Script | Block of Logic                   | Inputs/Outputs                                                                                                        | Key Functions/Classes         | Intra-File Connectivity                                                                                             | Inter-File Connectivity                                                                                                                        | Purpose/Description                                                                                                                                                                                    |
|---------------|-------------|----------------------------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| utils         |  job_gen.py  | Load Configuration Templates     | - Inputs: Configuration file path<br>- Outputs: Configuration templates                                               | Loading YAML file             | None                                                                                                                  | Utilizes configuration templates defined in `/conf/templates.yml`                                                                               | Loads job configuration templates which define the structure and parameters for various data engineering and machine learning jobs.                                                                    |
| utils         | job_gen.py  | Load Project Code                | - Inputs: Base configuration file path<br>- Outputs: Project code                                                     | Loading YAML file             | None                                                                                                                  | Relies on the base configuration defined in `/conf/base.yml`                                                                                     | Loads the project code from the base configuration, which is a unique identifier for the project used in various parts of the codebase.                                                                |
| utils         | job_gen.py  | Discover Models and Regions      | - Inputs: Model configuration base path<br>- Outputs: List of model and region combinations                            | `DiscoverHandler.discover_models()` | None                                                                                                                  | Uses model configuration files stored in `/conf/models/` directory                                                                               | Identifies all the models and regions configured in the project. This information is used to generate specific jobs for each model and region combination.                                              |
| utils         | job_gen.py  | Initialize Job Factory           | - Inputs: Configuration templates, models, and regions<br>- Outputs: JobFactory instance                               | `JobFactory` constructor     | Depends on the outputs from previous blocks                                                                          | `JobFactory` utilizes classes and methods from `dbx_utils.py` for job generation                                                                | Initializes a job factory that will be responsible for creating Databricks jobs based on the provided templates, models, and regions.                                                                   |
| utils         | job_gen.py  | Generate Jobs for Environments   | - Inputs: Project code, environment names (dev, staging, prod)<br>- Outputs: Jobs for each environment                 | `JobFactory.build_jobs()`    | Utilizes `JobFactory` instantiated earlier                                                                           | Generated jobs might reference tasks and configurations defined in other scripts and configuration files in the `/gda_clv` directory.           | Generates a series of Databricks jobs for different environments (development, staging, production) using the templates and configurations loaded previously.                                         |
| utils         | job_gen.py  | Save Deployment Configuration    | - Inputs: Generated jobs for environments<br>- Outputs: Deployment configuration file (`deployment.yml`)              | Writing YAML file            | Depends on the job generation process                                                                                | The resulting `deployment.yml` is used by Databricks for deploying and scheduling jobs as per the configurations.                              | Saves the generated job configurations for each environment into a YAML file, which will be used for deploying the jobs in Databricks.                                                                 |

This analysis of `/gda_clv/utils/job_gen.py` reveals the systematic process of generating Databricks jobs. The script starts by loading necessary configurations and identifying the models and regions involved. It then uses the `JobFactory` to create specific jobs for different environments based on these configurations. Finally, it saves these job configurations into a `deployment.yml` file, which is crucial for the deployment and management of these jobs in Databricks.





### Analysis of `/gda_clv/utils/job_meta_checker.py`

| Parent Folder | File/Script          | Block of Logic                     | Inputs/Outputs                                                                                                                                                                   | Key Functions/Classes       | Intra-File Connectivity                                                                                                                                                                       | Inter-File Connectivity                                                                                                          | Purpose/Description                                                                                                                                                                                                                                              |
|---------------|----------------------|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| utils         | job_meta_checker.py  | Imports and Notebook Context Setup | - Inputs: None<br>- Outputs: Import statements, JobContext initialization                                                                                                        | Import statements, `JobContext` | None                                                                                                                                                                                        | Relies on `JobContext` from `common.py` for setting up the context and obtaining environment and configuration details.           | Sets up the necessary imports and initializes the `JobContext`, which provides the context for the job including environment and model configuration details.                                                                                           |
| utils         | job_meta_checker.py  | Meta Check Commit Function         | - Inputs: Commit ID, Databricks notebook path<br>- Outputs: Validation result                                                                                                     | `meta_check_commit()`      | Depends on `JobContext` initialization                                                                                                                                                         | None                                                                                                                                 | Checks that each region model in the Databricks environment has a corresponding model with the current commit ID. This ensures version control and traceability of model deployments in different environments.                                          |
| utils         | job_meta_checker.py  | Main Function Execution            | - Inputs: None<br>- Outputs: Execution of meta-check functions                                                                                                                   | `meta_check_commit()` call | Calls `meta_check_commit()`                                                                                                                                                                   | None                                                                                                                                 | Orchestrates the execution of the meta-check process to validate the commit ID consistency across different models and regions in the Databricks environment.                                                                                          |

This analysis of `/gda_clv/utils/job_meta_checker.py` showcases its role in ensuring version control and consistency across model deployments in Databricks. The script sets up the necessary context and performs a commit ID check across all region models. This is crucial for maintaining traceability and alignment with the version control system, ensuring that the deployed models in Databricks match the current codebase version.





### Analysis of `/gda_clv/utils/job_templates_scheduler_gen.py`

| Parent Folder | File/Script                   | Block of Logic          | Inputs/Outputs                                                                                                                                                                    | Key Functions/Classes | Intra-File Connectivity                                                                                                 | Inter-File Connectivity | Purpose/Description                                                                                                                                                                                                                                                           |
|---------------|-------------------------------|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| utils         | job_templates_scheduler_gen.py | Imports and Functions   | - Inputs: None<br>- Outputs: Import statements, function definitions                                                                                                              | Import statements, function definitions like `time_to_trigger`, `get_all_job_schedule`, `get_updated_schedule` | None                                                                                                                       | None                    | Sets up the necessary imports and defines functions for job scheduling. Functions like `time_to_trigger` and `get_all_job_schedule` are crucial for converting datetime objects to cron job format and setting up schedules for different jobs.                             |
| utils         | job_templates_scheduler_gen.py | Schedule Generation     | - Inputs: Current time, job names, and their respective scheduling delays<br>- Outputs: Job scheduling parameters in cron format                                                    | `get_all_job_schedule` | Calls function `time_to_trigger` to convert datetime objects to cron job format for scheduling                          | None                    | Generates a schedule for various jobs based on the current time and predefined delays. This schedule is then converted to cron job format to be used in Databricks job scheduling.                                                                                           |
| utils         | job_templates_scheduler_gen.py | Updated Schedule        | - Inputs: Base templates, environment name, region name<br>- Outputs: Updated templates with new scheduling parameters                                                           | `get_updated_schedule` | Calls `get_all_job_schedule` to generate updated schedules                                                                | None                    | Dynamically updates the job schedule in the base templates with new timings. This is essential for ensuring that job execution is aligned with the current operational requirements and environment specifics.                                                                 |
| utils         | job_templates_scheduler_gen.py | Writing to YAML         | - Inputs: Updated templates<br>- Outputs: YAML file with updated job schedules                                                                                                    | File write operation   | None                                                                                                                       | None                    | Writes the updated job templates with the new schedules to a YAML file. This file is then used to configure job execution in Databricks, ensuring that jobs run according to the newly generated schedule.                                                                      |
| utils         | job_templates_scheduler_gen.py | Main Function Execution | - Inputs: Configuration file paths, environment and region names<br>- Outputs: Updated templates with new scheduling parameters, and the creation of a YAML file with these updates | Main execution block   | Calls functions `get_updated_schedule` and writes the output to a YAML file                                               | None                    | Orchestrates the entire process of generating new schedules for Databricks jobs, updating the job templates, and writing the updated configuration to a YAML file. This ensures that job execution in Databricks is properly scheduled and aligned with the project's needs. |

The script `/gda_clv/utils/job_templates_scheduler_gen.py` plays a crucial role in dynamically generating and updating the schedules for Databricks jobs. It ensures that job execution is timely and aligned with the current project requirements, making it an essential part of the operational workflow in the `gda-clv` project.





### Analysis of `/gda_clv/utils/logger_utils.py`

| Parent Folder | File/Script         | Block of Logic        | Inputs/Outputs                                      | Key Functions/Classes | Intra-File Connectivity | Inter-File Connectivity                              | Purpose/Description                                                                                                          |
|---------------|---------------------|-----------------------|-----------------------------------------------------|-----------------------|-------------------------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| utils         | logger_utils.py     | Imports and Functions | - Inputs: None<br>- Outputs: Import statements, function definitions | Import `logging` module, `get_logger` function definition | None                      | None                                                  | Sets up the necessary imports for logging and defines the `get_logger` function for creating a logger with a specific format. |
| utils         | logger_utils.py     | Logger Configuration  | - Inputs: None<br>- Outputs: Configured logger object             | `get_logger`           | None                      | Referenced in other scripts where logging is required | Configures and returns a logger object with a predefined format, useful for consistent logging across different scripts.    |

The script `/gda_clv/utils/logger_utils.py` is focused on setting up and configuring a logger for the `gda-clv` project. It establishes a standard logging format, which is essential for maintaining clear and consistent logging practices across various parts of the project. The `get_logger` function can be utilized by different scripts within the project to ensure uniformity in logging outputs, aiding in effective monitoring and debugging.